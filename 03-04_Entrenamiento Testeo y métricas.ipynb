{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92c6fb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Librerías de la limpieza\n",
    "import re\n",
    "from stop_words import get_stop_words\n",
    "import unicodedata\n",
    "from num2words import num2words\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, precision_recall_curve\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2314c867",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../dataset'\n",
    "\n",
    "pet_json = 'reviews_Pet_Supplies_5.json.gz'\n",
    "\n",
    "pet = pd.read_json(os.path.join(dataset_path, pet_json), lines=True, compression='gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd56d0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar los datos (codificar puntuaciones y seleccionar 10000 registros de cada grupo y ordenarlos al azar)\n",
    "def prepare_data(df):\n",
    "    df = df.dropna(subset=['reviewText', 'overall'])[['reviewText', 'overall']]\n",
    "    df[\"overall\"].replace({1: 0, 2: 0, 3: 0, 4: 1, 5: 1}, inplace=True)\n",
    "    \n",
    "    positive = df[df[\"overall\"] > 0]\n",
    "    negative = df[df[\"overall\"] < 1]\n",
    "    \n",
    "    positive = positive[0:10000]\n",
    "    negative = negative[0:10000]\n",
    "    \n",
    "    df = pd.concat([positive, negative], axis=0)\n",
    "    \n",
    "    dataframe = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def lower(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_characters(text):\n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    text = re.sub(r'[0-9]+', ' ', text)\n",
    "    \n",
    "    return text \n",
    "\n",
    "def eliminate_stopwords(text):\n",
    "    sw_list = get_stop_words('en')\n",
    "    text = ' '.join([word for word in text.split() if word not in sw_list])\n",
    "    \n",
    "    return text\n",
    "\n",
    "def lemmatization(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_text(text, split=False):\n",
    "    text = lower(text)\n",
    "    text = clean_characters(text)\n",
    "    text = eliminate_stopwords(text)\n",
    "    text = lemmatization(text)\n",
    "    if split:\n",
    "        text = text.split()\n",
    "    else:\n",
    "        pass\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "# Juntamos las funciones que trabajan sobre cada una de las columnas\n",
    "def cleaning(df, split=False):\n",
    "    reviews = []\n",
    "    \n",
    "    for text in df.reviewText:\n",
    "        reviews.append(clean_text(text, split))\n",
    "    \n",
    "    sentiment = []\n",
    "    for item in df.overall:\n",
    "        if item > 0:\n",
    "            sentiment.append(1)\n",
    "        else:\n",
    "            sentiment.append(0)\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "    'review': reviews,\n",
    "    'sentiment': sentiment\n",
    "    })\n",
    "\n",
    "    df.dropna(subset=['review', 'sentiment'], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Juntamos todas las funciones\n",
    "def transform_df_to_text(df, split):\n",
    "    a = prepare_data(df)\n",
    "    df = cleaning(a, split)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69e2b6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pet = transform_df_to_text(pet, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92c95775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[made, mistake, ordering, little, jacs, bit, l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[absolutely, love, product, first, bought, dog...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[thing, always, falling, bottom, tank, reach, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  [made, mistake, ordering, little, jacs, bit, l...          0\n",
       "1  [absolutely, love, product, first, bought, dog...          1\n",
       "2  [thing, always, falling, bottom, tank, reach, ...          0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pet[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "326cdbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pet2 = transform_df_to_text(pet, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc0fa1b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>job suppose recommend extra support use fluval...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bought two trying two brand unit three advanta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>first bought diffuser every room house recomme...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  job suppose recommend extra support use fluval...          1\n",
       "1  bought two trying two brand unit three advanta...          1\n",
       "2  first bought diffuser every room house recomme...          0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pet2[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a2a3ec",
   "metadata": {},
   "source": [
    "# **1. Método Clásico: Regresión logística**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2b6f67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separación train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_pet2['review'],\n",
    "    df_pet2['sentiment'],\n",
    "    train_size=0.75,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89e68956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_df=0.95, max_features=23370, min_df=0.05)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracción de features\n",
    "cv = TfidfVectorizer(\n",
    "    max_df=0.95,\n",
    "    min_df=0.05,\n",
    "    max_features=23370,\n",
    "    ngram_range=(1, 1)\n",
    ")\n",
    "cv.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efa6130",
   "metadata": {},
   "source": [
    "Para el vectorizer se han escogido estos parámetros debido a:\n",
    "\n",
    "1- *max_df* y *min_df*: eliminamos las palabras con una frecuencia excesivamente alta y baja.\n",
    "\n",
    "2- *max_features*: Considera las 5000 palabras más frecuentes únicamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "445952ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF IDF\n",
    "X_train_ = cv.transform(X_train)\n",
    "X_test_ = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4401ee29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.05: 0.7204\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C=0.05, solver='lbfgs', max_iter=500)\n",
    "lr.fit(X_train_, y_train)\n",
    "    \n",
    "train_predict = lr.predict(X_train_)\n",
    "test_predict = lr.predict(X_test_)\n",
    "    \n",
    "print (\"Accuracy for C=0.05: {}\".format(accuracy_score(y_test, test_predict)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548db474",
   "metadata": {},
   "source": [
    "# **2. Gradient Boosting**\n",
    "\n",
    "**2.1. Bag of Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7766e3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separación train/test\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(\n",
    "    df_pet['review'],\n",
    "    df_pet['sentiment'],\n",
    "    train_size=0.75,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a9d664d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import joblib\n",
    "\n",
    "# Bag of Words.\n",
    "def extract_BoW_features(words_train, words_test, vocabulary_size):\n",
    "    vectorizer = CountVectorizer(max_features=vocabulary_size,\n",
    "            preprocessor=lambda x: x, tokenizer=lambda x: x)  # already preprocessed\n",
    "    features_train = vectorizer.fit_transform(words_train).toarray()\n",
    "\n",
    "    features_test = vectorizer.transform(words_test).toarray()\n",
    "                \n",
    "    vocabulary = vectorizer.vocabulary_\n",
    "    \n",
    "    return features_train, features_test, vocabulary\n",
    "\n",
    "features_train, features_test, vocabulary = extract_BoW_features(X_train2, X_test2, 23370)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ae972b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as pr\n",
    "\n",
    "features_train = pr.normalize(features_train, axis=1)\n",
    "features_test = pr.normalize(features_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268e3467",
   "metadata": {},
   "source": [
    "**2.2 Gradient Boosting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bd84483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GradientBoostingClassifier] Accuracy: train = 0.7404, test = 0.723\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "n_estimators = 32\n",
    "\n",
    "def classify_gboost(X_train, X_test, y_train, y_test):        \n",
    "    clf = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=1.0, max_depth=1, random_state=42)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"[{}] Accuracy: train = {}, test = {}\".format(\n",
    "            clf.__class__.__name__,\n",
    "            clf.score(X_train, y_train),\n",
    "            clf.score(X_test, y_test)))\n",
    "    \n",
    "    return clf, clf.score(X_train, y_train), clf.score(X_test, y_test)\n",
    "\n",
    "\n",
    "clf2, score_train, score_test = classify_gboost(features_train, features_test, y_train2, y_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db4229c",
   "metadata": {},
   "source": [
    "Los Accuracy son prácticamente iguales, por lo que no parece que haya overfitting o underfitting. Debido a la gran cantidad de tiempo que lleva este proceso, lo mejor es hacer este procedimiento pero usando deep learning para evitar el pre-procesamiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2783e7",
   "metadata": {},
   "source": [
    "# **3. Red Neuronal**\n",
    "\n",
    "**GRU**\n",
    "\n",
    "Al haber reducido los datos que estoy utilizando he optado por utilizar GRU sobre LSTM ya que suele funcionar mejor con conjuntos de datos más pequeños."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79ee4749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos el tamaño del vocabulario y la longitud de la frase más larga.\n",
    "\n",
    "vocab = dict()\n",
    "maxlen = 0\n",
    "\n",
    "for l in df_pet.review:\n",
    "    for item in l:\n",
    "        if item in vocab:\n",
    "            vocab[item] += 1\n",
    "        else:\n",
    "            vocab.update({item : 1})\n",
    "\n",
    "for l in df_pet.review:\n",
    "    if len(l) > maxlen:\n",
    "        maxlen = len(l)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "beeaf91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---[Tamaño del vocabulario]---\n",
      "23370\n",
      "---[Mayor longitud de frase]---\n",
      "770\n"
     ]
    }
   ],
   "source": [
    "print(\"---[Tamaño del vocabulario]---\")\n",
    "print(len(vocab))\n",
    "print(\"---[Mayor longitud de frase]---\")\n",
    "print(maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "535daeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hay que transformar cada palabra en un número, para ello tomaré las keys del diccionario, las asignaré un número único y cambiaré todas las palabras del df\n",
    "n = 0\n",
    "vocab_uniq = dict()\n",
    "reviews_plane = []\n",
    "\n",
    "for key in vocab:\n",
    "    vocab_uniq.update({key : n})\n",
    "    n += 1\n",
    "\n",
    "for l in df_pet.review:\n",
    "    l2 = []\n",
    "    for item in l:\n",
    "        l2.append(vocab_uniq[item])\n",
    "    l2 = np.array(l2)\n",
    "    l2 = tf.convert_to_tensor(l2)\n",
    "    reviews_plane.append(l2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "717a50c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\"review\" : reviews_plane, \"sentiment\" : df_pet.sentiment})\n",
    "\n",
    "# Separación train/test\n",
    "X_trainDL, X_testDL, y_trainDL, y_testDL = train_test_split(\n",
    "    data['review'],\n",
    "    data['sentiment'],\n",
    "    train_size=0.75,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_words = maxlen\n",
    "\n",
    "X_trainDL = sequence.pad_sequences(X_trainDL, maxlen=max_words)\n",
    "X_testDL = sequence.pad_sequences(X_testDL, maxlen=max_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "420d97e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 770, 64)           1495680   \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 100)               49800     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,545,581\n",
      "Trainable params: 1,545,581\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, GRUV2, SimpleRNN\n",
    "\n",
    "vocabulary_size = len(vocab)\n",
    "max_words = maxlen\n",
    "\n",
    "embedding_size = 64\n",
    "model_gru = Sequential()\n",
    "model_gru.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "model_gru.add(GRUV2(100))\n",
    "model_gru.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model_gru.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a701706",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60d83667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model_gru, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1622a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "234/234 [==============================] - 206s 868ms/step - loss: 0.5489 - accuracy: 0.7178 - val_loss: 0.5395 - val_accuracy: 0.6719\n",
      "Epoch 2/2\n",
      "234/234 [==============================] - 225s 962ms/step - loss: 0.3388 - accuracy: 0.8564 - val_loss: 0.5339 - val_accuracy: 0.7031\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 2\n",
    "\n",
    "X_validDL, y_validDL = X_trainDL[:batch_size], y_trainDL[:batch_size]  # first batch_size samples\n",
    "X_trainDL2, y_trainDL2 = X_trainDL[batch_size:], y_trainDL[batch_size:]  # rest for training\n",
    "\n",
    "modelDL = model_gru.fit(X_trainDL2, y_trainDL2,\n",
    "          validation_data=(X_validDL, y_validDL),\n",
    "          batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fd6ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = model_gru.evaluate(X_testDL, y_testDL, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89074399",
   "metadata": {},
   "source": [
    "Vemos que la accuracy es bastante alta, aunque el error también lo es considerando que se trata de un problema binario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3627f058",
   "metadata": {},
   "source": [
    "# **4. Métricas**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6963cd7",
   "metadata": {},
   "source": [
    "**Comparación final y conclusiones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f48ff3c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "--- [Regresión logística] ---\n",
      "La precisión de este modelo ha sido de: [0.7204]\n",
      "\r\n",
      "--- [Gradient Boosting] ---\n",
      "La precisión de este modelo ha sido de: [0.723] \n",
      "\r\n",
      "--- [Deep Learning: GRU] ---\n",
      "La precisión de este modelo ha sido de: [0.81],\r\n",
      "La función de pérdida de este ha sido:  [0.43]\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\r\\n--- [Regresión logística] ---\")\n",
    "print (\"La precisión de este modelo ha sido de: [{}]\".format(accuracy_score(y_test, test_predict)))\n",
    "print(\"\\r\\n--- [Gradient Boosting] ---\")\n",
    "print (\"La precisión de este modelo ha sido de: [{}] \".format(score_test))\n",
    "print(\"\\r\\n--- [Deep Learning: GRU] ---\")\n",
    "print(\"La precisión de este modelo ha sido de: [{:.2f}],\\r\\nLa función de pérdida de este ha sido:  [{:.2f}]\\r\\n\".format(evaluation[1], evaluation[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d607c5",
   "metadata": {},
   "source": [
    "La precisión de los primeros modelos ha sido de 0.7~. En el caso del último modelo se ha obtenido una precisión de 0.81, mucho mejor que los anteriores, no obstante la función de pérdida indica que no es un modelo muy optimizado.\n",
    "\n",
    "Pese a esto, ninguno de los modelos muestra señales de overfitting o underfitting a simple vista."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460967ff",
   "metadata": {},
   "source": [
    "La precisión ha podido ser menor en los modelos más clásicos debido a la menor complejidad de estos. Tratándose de un problema complejo como es determinar la puntuación que los clientes han puesto a los productos a partir de la reseña, parece lógico pensar que modelos más complejos podrían obtener mejores resultados.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
